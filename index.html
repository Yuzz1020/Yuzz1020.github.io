<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EWSWCE7MTD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EWSWCE7MTD');
</script>


  <title>Zhongzhi Yu</title>
  
  <meta name="author" content="Zhongzhi Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gt_seal.png">
</head>

<body>
  <table style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhongzhi Yu</name>&nbsp&nbsp;
              </p>
              <p>I am a 4th-year Ph.D. student at Georgia Tech, advised by <a href="https://eiclab.scs.gatech.edu/pages/team.html">Prof. Yingyan (Celine) Lin</a>. My research interests are in efficient deep learning algorithms with a focus on designing efficient training and tuning method for large-scale models. </p>
              <p>Before starting my Ph.D. journey, I receieved my M.S. from Columbia university and my B.Eng. from Zhejiang University. </p>
              <p>I have been interned as a researhc intern at MIT-IBM Watson AI Lab during 2021, where I was fortunate to work with <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/">Dr. Yang Zhang</a> and <a href="https://www.linkedin.com/in/kaizhi-qian-4228ba6a/">Dr. Kaizhi Qian</a>.</p>
              <p style="text-align:center">
              	<a href="zyu401@gatech.edu">Email</a>&nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=KjvcaBQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="data/Zhongzhi_s_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zhongzhi-yu/">LinkedIn</a> &nbsp/&nbsp
              </p><br>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="data/Zhongzhi_Img.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="data/Zhongzhi_Img.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
          <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>  
                <div style="height:200px;overflow-y:auto">
                <p></p><ul>
                <li> [Jul. 2023] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://arxiv.org/pdf/2309.10730.pdf"><papertitle>GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models</papertitle></a> accepted at <strong>ICCAD 2023</strong>.</li>
                <li> [Jul. 2023] <b style="color:#3EA055;">[Award]</b> Our Demo <papertitle>Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via Algorithm-Hardware Co-Design</papertitle> is awarded 2nd Place on University Demo Best Demonstration Award <strong>at DAC 2023</strong>.</li>
                <li> [Apr. 2023] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://arxiv.org/abs/2306.15686"><papertitle>Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modularized Learning</papertitle></a> accepted at <strong>ICML 2023</strong>.</li>
                <li> [Feb. 2023] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf"><papertitle>Hint-Aug: Drawing Hints from Vision Foundation Models towards Boosted Few-shot Parameter-Efficient ViT Tuning</papertitle></a> accepted at <strong>CVPR 2023</strong>.</li>
                <li> [Feb. 2023] <b style="color:#3EA055;">[Paper]</b> Our work <a href="https://arxiv.org/pdf/2306.13586.pdf"><papertitle>NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants</papertitle></a> accepted at <strong>DAC 2023</strong>.</li>
              </ul>
          </div>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:101%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research Interests</heading>  
                <p></p>
                My research interests are in efficient deep learning algorithms with a focus on designing efficient training and tuning method for large-scale models. Recently, I focus on enabling the large langauge models tuning on resource-constrained devices using algorithm and accelerator co-design techniques, enabling more accessible and deployable customized large language models across a wider range of applications. 
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>
       
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>  &nbsp &nbsp &nbsp (*: Equal Contributions)
            </td> 
          </tr> 
        </tbody></table> 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/GPT4AIGchip.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models</papertitle>
              <br>
              Yonggan Fu*, Yongan Zhang*, Zhongzhi Yu*, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan (Celine) Lin
              <br>
              <a href="https://arxiv.org/pdf/2309.10730.pdf">Paper</a> 
              <br>
              <p></p>
              <p>We develop GPT4AIGChip, a framework intended to democratize AI accelerator design by leveraging human natural languages instead of domain-specific languages.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/master_asr.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modularized Learning</papertitle>
              <br>
              Zhongzhi Yu, Yang Zhang, Kaizhi Qian, Cheng Wan, Yonggan Fu, Yongan Zhang, and Yingyan (Celine) Lin
              <br>
              <a href="https://arxiv.org/abs/2306.15686">Paper</a> 
              <br>
              <p></p>
              <p>We propose an ASR framework, dubbed Master-ASR, that, for the first time, simultaneously achieves strong multilingual scalability and low-resource adaptation ability in a modularized-then-assemble manner.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Hint-Aug.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Hint-Aug: Drawing Hints from Vision Foundation Models towards Boosted Few-shot Parameter-Efficient ViT Tuning</papertitle>
              <br>
              Zhongzhi Yu, Shang Wu, Yonggan Fu, Cheng Wan, Shunyao Zhang, Chaojian Li, and Yingyan (Celine) Lin
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf">Paper</a> 
              <br>
              <p></p>
              <p>We propose a framework called Hint-based Data Augmentation (Hint-Aug), which is dedicated to boosting the effectiveness of few-shot tuning foundation ViT models.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/netbooster.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants</papertitle>
              <br>
              Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, and Yingyan (Celine) Lin
              <br>
              <a href="https://arxiv.org/pdf/2306.13586.pdf">Paper</a> 
              <br>
              <p></p>
              <p>We propose a framework called NetBooster to empower tiny deep learning by augmenting the architectures of TNNs via an expansion-then-contraction strategy.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eyecod_tp.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm/Hardware Co-Design</papertitle>
              <br>
              Haoran You*, Cheng Wan*, Yang Zhao*, Zhongzhi Yu*, Yonggan Fu, Jiayi Yuan, Shang Wu, Shunyao Zhang, Yongan Zhang, Chaojian Li, Vivek Boominathan, Ashok Veeraraghavan, Ziyun Li, and Yingyan (Celine) Lin
              <br>
              <a href="https://ieeexplore.ieee.org/document/10123119">Paper</a> 
              <br>
              <p></p>
              <p>We devise a lensless FlatCam-based eye tracking algorithm and hardware accelerator co-design framework dubbed EyeCoD, which is the first system to meet the high throughput requirement with smaller form-factor.</p>
            </td>
          </tr> 
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AugViT-1.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>AugViT: Improving Vision Transformer Training by Marrying Attention and Data Augmentation</papertitle>
              <br>
              Zhongzhi Yu, Yonggan Fu, Chaojian Li, and Yingyan (Celine) Lin
              <br>
              <a href="https://ieeexplore.ieee.org/document/10123119">Paper</a> 
              <br>
              <p></p>
              <p>We propose a data augmentation framework called AugViT, which is dedicated to incorporating the key component in ViTs, i.e., self-attention, into data augmentation intensity to enable ViT's outstanding accuracy across various devices.</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/miaformer.png" alt="clean-usnob" width="150" height="150">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation</papertitle>
              <br>
              Zhongzhi Yu, Yonggan Fu, Sicheng Li, Mengquan Li, Chaojian Li, and Yingyan (Celine) Lin
              <br>
              <a href="https://www.aaai.org/AAAI22Papers/AAAI-4994.YuZ.pdf">Paper</a> 
              <br>
              <p></p>
              <p>We propose a Multi-grained Input-Adaptive Vision Transformer framework dubbed MIA-Former that can input-adaptively adjust the structure of ViTs at three coarse-to-fine-grained granularities (i.e., model depth and the number of model heads/tokens).</p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/LDP.png" alt="clean-usnob" width="160" height="160">
              </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>LDP: Learnable Dynamic Precision for Efficient Deep Neural Network Training and Inference</papertitle>
              <br>
              Zhongzhi Yu, Yonggan Fu, Shang Wu, Mengquan Li, Haoran You, and Yingyan Lin
              <br>
              <a href="https://arxiv.org/abs/2203.07713">Paper</a> 
              <br>
              <p></p>
              <p>We propose LDP, a Learnable Dynamic Precision DNN training framework that can automatically learn a temporally and spatially dynamic precision schedule during training towards optimal accuracy and efficiency trade-offs.</p>
            </td>
          </tr> 

          </tbody>
          </table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service</heading>  
 				<p></p><ul>
 				<li> Conference Reviewer: ICLR'23, NeurIPS'23, ICML'23, CVPR'23, AAAI'22, NeurIPS'22, AICAS'22.</li>
 				<li> Artifact Evaluation Committee: MICRO'23.</li>
   				</ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last Update: Sep. 2023
                <br><br>
                <a href="https://github.com/jonbarron/website">Website Template</a>
                </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
