<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EWSWCE7MTD"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-EWSWCE7MTD');
  </script>

  <title>Zhongzhi Yu – Research Scientist (LLMs, Efficient DL)</title>
  <meta name="description" content="Zhongzhi Yu—Research Scientist at NVIDIA; Ph.D. (Georgia Tech). Efficient LLM inference/tuning, transformer adaptation, and LLM-for-hardware. Publications, CV, and contact.">
  <meta name="keywords" content="Zhongzhi Yu, Research Scientist, NVIDIA, LLM, Efficient Deep Learning, Georgia Tech, Large Language Models, Transformer, AI, Machine Learning">
  <link rel="canonical" href="https://yuzz1020.github.io/" />
  <meta name="robots" content="index,follow" />

  <meta name="author" content="Zhongzhi Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gt_seal.png">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Zhongzhi Yu",
    "alternateName": ["Dr. Zhongzhi Yu", "Zhongzhi Yu NVIDIA", "Zhongzhi Yu Georgia Tech"],
    "affiliation": {
      "@type": "Organization",
      "name": "NVIDIA Research"
    },
    "alumniOf": [
      {
        "@type": "Organization",
        "name": "Georgia Institute of Technology"
      },
      {
        "@type": "Organization",
        "name": "Columbia University"
      },
      {
        "@type": "Organization",
        "name": "Zhejiang University"
      }
    ],
    "jobTitle": "Research Scientist",
    "description": "Research Scientist at NVIDIA specializing in Large Language Models (LLMs), Efficient Deep Learning, and AI Accelerator Design",
    "knowsAbout": ["Large Language Models", "Efficient Deep Learning", "Transformer Models", "AI Accelerators", "Computer Vision"],
    "url": "https://yuzz1020.github.io/",
    "sameAs": [
      "https://scholar.google.com/citations?user=KjvcaBQAAAAJ",
      "https://www.linkedin.com/in/zhongzhi-yu/",
      "https://github.com/Yuzz1020"
    ],
    "email": "mailto:zyu401@gatech.edu"
  }
  </script>
</head>

<body>
  <a class="skip-link" href="#main">Skip to main content</a>
  <div class="background-layers" aria-hidden="true">
    <div class="orb orb-1"></div>
    <div class="orb orb-2"></div>
    <div class="orb orb-3"></div>
    <div class="grid-overlay"></div>
  </div>

  <header class="site-header" id="top">
    <div class="top-nav" role="navigation" aria-label="Primary">
      <div class="nav-wrapper">
        <div class="brand" aria-label="Zhongzhi Yu personal mark">ZY</div>
        <nav class="site-nav" aria-label="Primary">
          <a href="#news">News</a>
          <a href="#research">Research</a>
          <a href="#publications">Publications</a>
          <a href="#service">Service</a>
          <a href="data/Zhongzhi_Yu_CV.pdf" class="nav-cta" target="_blank" rel="noopener">CV</a>
        </nav>
      </div>
    </div>

    <div class="hero container">
      <div class="hero-copy">
        <p class="eyebrow">Research Scientist @ NVIDIA</p>
        <h1>Zhongzhi Yu</h1>
        <p class="lead">
          I design efficient learning algorithms for large language models, with a focus on inference calibration,
          adaptive tuning, and human-in-the-loop hardware design. My work bridges LLM foundations with practical
          deployment on data- and compute-constrained platforms.
        </p>
        <div class="hero-tags" role="list">
          <span class="tag" role="listitem">LLM Efficiency</span>
          <span class="tag" role="listitem">Attention Calibration</span>
          <span class="tag" role="listitem">LLM-for-Hardware</span>
        </div>
        <div class="hero-actions" aria-label="Primary contact links">
          <a href="mailto:zyu401@gatech.edu" class="button">Email</a>
          <a href="https://scholar.google.com/citations?user=KjvcaBQAAAAJ&hl=en" class="button ghost" target="_blank" rel="noopener">Google Scholar</a>
          <a href="https://www.linkedin.com/in/zhongzhi-yu/" class="button ghost" target="_blank" rel="noopener">LinkedIn</a>
          <a href="https://github.com/Yuzz1020" class="button ghost" target="_blank" rel="noopener">GitHub</a>
        </div>
        <div class="hero-focus" role="presentation">
          <p>Currently, I'm exploring on-the-fly inference upgrades for foundation models and co-designing AI accelerators with LLM assistance.</p>
        </div>
      </div>
      <aside class="hero-profile">
        <figure class="profile-frame">
          <img src="data/Zhongzhi_Img.jpg" alt="Portrait of Zhongzhi Yu" loading="lazy">
        </figure>
        <div class="profile-card">
          <h2>Academic Roots</h2>
          <p>Previously, I earned my Ph.D. in Computer Science from Georgia Tech, advised by
            <a href="https://eiclab.scs.gatech.edu/pages/team.html" target="_blank" rel="noopener">Prof. Yingyan (Celine) Lin</a>.
          </p>
          <p>I also hold an M.S. from Columbia University and a B.Eng. from Zhejiang University, and I have collaborated with MIT-IBM Watson AI Lab.</p>
        </div>
      </aside>
    </div>
  </header>

  <main id="main">
    <section class="section" id="news">
      <div class="container">
        <header class="section-header">
          <p class="section-kicker">Signal Log</p>
          <h2>News & Highlights</h2>
          <p>Recent milestones, publications, and recognitions.</p>
        </header>
        <div class="news-timeline" role="feed">
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Jun 2024</span>
              <span class="badge award">Award</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/pdf/2407.01910" target="_blank" rel="noopener">MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation</a></h3>
              <p>Received the Best Paper Award at the inaugural IEEE LAD 2024 workshop on LLM-Aided Design.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">May 2024</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/pdf/2406.15765" target="_blank" rel="noopener">Unveiling and Harnessing Hidden Attention Sinks</a></h3>
              <p>Our attention calibration framework for LLMs has been accepted to ICML 2024.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Feb 2024</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/pdf/2406.15758" target="_blank" rel="noopener">EDGE-LLM</a></h3>
              <p>Layer-wise compression and adaptive tuning techniques for on-device LLM adaptation accepted by DAC 2024.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Jul 2023</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/pdf/2309.10730.pdf" target="_blank" rel="noopener">GPT4AIGChip</a></h3>
              <p>Our LLM-driven accelerator design framework has been accepted by ICCAD 2023.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Jul 2023</span>
              <span class="badge award">Award</span>
            </div>
            <div class="news-content">
              <h3>Gen-NeRF Demo</h3>
              <p>Won 2nd place in the University Demo Best Demonstration Award at DAC 2023.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Apr 2023</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/abs/2306.15686" target="_blank" rel="noopener">Master-ASR</a></h3>
              <p>Modularized multilingual ASR system accepted by ICML 2023.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Feb 2023</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf" target="_blank" rel="noopener">Hint-Aug</a></h3>
              <p>Few-shot ViT tuning framework accepted by CVPR 2023.</p>
            </div>
          </article>
          <article class="news-item" role="article">
            <div class="news-meta">
              <span class="news-date">Feb 2023</span>
              <span class="badge publication">Paper</span>
            </div>
            <div class="news-content">
              <h3><a href="https://arxiv.org/pdf/2306.13586.pdf" target="_blank" rel="noopener">NetBooster</a></h3>
              <p>Efficiency boosting framework for tiny neural networks accepted by DAC 2023.</p>
            </div>
          </article>
        </div>
      </div>
    </section>

    <section class="section surface" id="research">
      <div class="container">
        <header class="section-header">
          <p class="section-kicker">Research Thesis</p>
          <h2>Research Focus</h2>
        </header>
        <div class="research-grid">
          <div class="research-overview">
            <p>
              I study how to make large language models adaptable, efficient, and dependable in real-world settings.
              My research spans three pillars:
            </p>
            <div class="pillar-grid" role="list">
              <article class="pillar-card" role="listitem">
                <h3>Inference Calibration</h3>
                <p><strong>Inference Calibration:</strong> Elevating LLM outputs with lightweight attention refinements and dynamic evaluation strategies.</p>
              </article>
              <article class="pillar-card" role="listitem">
                <h3>Adaptive Tuning</h3>
                <p><strong>Adaptive Tuning:</strong> Designing compression-aware fine-tuning and voting schemes so edge devices can host powerful LLM experiences.</p>
              </article>
              <article class="pillar-card" role="listitem">
                <h3>LLM-for-Hardware</h3>
                <p><strong>LLM-for-Hardware:</strong> Co-designing silicon and software by steering LLMs to generate and verify hardware designs with human-centered workflows.</p>
              </article>
            </div>
            <p>
              Previously, I earned my Ph.D. in Computer Science from Georgia Tech, advised by
              <a href="https://eiclab.scs.gatech.edu/pages/team.html" target="_blank" rel="noopener">Prof. Yingyan (Celine) Lin</a>.
              I also hold an M.S. from Columbia University and a B.Eng. from Zhejiang University, and I have collaborated with MIT-IBM Watson AI Lab.
            </p>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="publications">
      <div class="container">
        <header class="section-header">
          <p class="section-kicker">Selected Work</p>
          <h2>Selected Publications</h2>
          <p class="subtitle">* denotes equal contribution</p>
        </header>
        <div class="publication-grid">
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/act.png" alt="Visualization for attention calibration technique">
            </div>
            <div class="card-body">
              <h3 class="paper-title">Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration</h3>
              <p class="authors">Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, Yingyan (Celine) Lin</p>
              <p class="description">A systematic study of attention sink behaviors in LLMs paired with an inference-time calibration method.</p>
              <a href="https://arxiv.org/pdf/2406.15765" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/edge-llm.gif" alt="Visualization for EDGE-LLM">
            </div>
            <div class="card-body">
              <h3 class="paper-title">EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting</h3>
              <p class="authors">Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang (Katie) Zhao, Yingyan (Celine) Lin</p>
              <p class="description">Compression-aware fine-tuning that unlocks on-device LLM adaptation with minimal hardware overhead.</p>
              <a href="https://arxiv.org/pdf/2406.15758" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/mg-verilog.png" alt="Visualization for MG-Verilog dataset">
            </div>
            <div class="card-body">
              <h3 class="paper-title">MG-Verilog: Multi-grained Dataset Towards Enhanced LLM-assisted Verilog Generation</h3>
              <p class="authors">Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, Yingyan (Celine) Lin</p>
              <p class="description">The first multi-granularity Verilog dataset enabling precise LLM-guided hardware code generation.</p>
              <a href="https://arxiv.org/pdf/2407.01910.pdf" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/GPT4AIGchip.png" alt="Visualization for GPT4AIGChip framework">
            </div>
            <div class="card-body">
              <h3 class="paper-title">GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models</h3>
              <p class="authors">Yonggan Fu*, Yongan Zhang*, Zhongzhi Yu*, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, Yingyan (Celine) Lin</p>
              <p class="description">Human-in-the-loop workflows that translate natural language into accelerator design artifacts.</p>
              <a href="https://arxiv.org/pdf/2309.10730.pdf" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/master_asr.png" alt="Visualization for Master-ASR framework">
            </div>
            <div class="card-body">
              <h3 class="paper-title">Master-ASR: Achieving Multilingual Scalability and Low-Resource Adaptation in ASR with Modularized Learning</h3>
              <p class="authors">Zhongzhi Yu, Yang Zhang, Kaizhi Qian, Cheng Wan, Yonggan Fu, Yongan Zhang, Yingyan (Celine) Lin</p>
              <p class="description">A modular ASR architecture that balances multilingual performance and low-resource specialization.</p>
              <a href="https://arxiv.org/abs/2306.15686" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/Hint-Aug.png" alt="Visualization for Hint-Aug">
            </div>
            <div class="card-body">
              <h3 class="paper-title">Hint-Aug: Drawing Hints from Vision Foundation Models towards Boosted Few-shot Parameter-Efficient ViT Tuning</h3>
              <p class="authors">Zhongzhi Yu, Shang Wu, Yonggan Fu, Cheng Wan, Shunyao Zhang, Chaojian Li, Yingyan (Celine) Lin</p>
              <p class="description">Integrating attention-aware data augmentation to amplify few-shot ViT tuning effectiveness.</p>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Hint-Aug_Drawing_Hints_From_Foundation_Vision_Transformers_Towards_Boosted_Few-Shot_CVPR_2023_paper.pdf" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/netbooster.png" alt="Visualization for NetBooster">
            </div>
            <div class="card-body">
              <h3 class="paper-title">NetBooster: Empowering Tiny Deep Learning By Standing on the Shoulders of Deep Giants</h3>
              <p class="authors">Zhongzhi Yu, Yonggan Fu, Jiayi Yuan, Haoran You, Yingyan (Celine) Lin</p>
              <p class="description">A blueprint for uplifting compact neural networks via expansion-then-contraction strategies.</p>
              <a href="https://arxiv.org/pdf/2306.13586.pdf" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/eyecod_tp.png" alt="Visualization for EyeCoD system">
            </div>
            <div class="card-body">
              <h3 class="paper-title">EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm/Hardware Co-Design</h3>
              <p class="authors">Haoran You*, Cheng Wan*, Yang Zhao*, Zhongzhi Yu*, Yonggan Fu, Jiayi Yuan, Shang Wu, Shunyao Zhang, Yongan Zhang, Chaojian Li, Vivek Boominathan, Ashok Veeraraghavan, Ziyun Li, Yingyan (Celine) Lin</p>
              <p class="description">A compact lensless eye-tracking system achieving high throughput through algorithm-hardware co-design.</p>
              <a href="https://ieeexplore.ieee.org/document/10123119" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/AugViT-1.png" alt="Visualization for AugViT">
            </div>
            <div class="card-body">
              <h3 class="paper-title">AugViT: Improving Vision Transformer Training by Marrying Attention and Data Augmentation</h3>
              <p class="authors">Zhongzhi Yu, Yonggan Fu, Chaojian Li, Yingyan (Celine) Lin</p>
              <p class="description">An attention-aware augmentation framework that consistently lifts ViT accuracy across hardware tiers.</p>
              <a href="https://ieeexplore.ieee.org/document/10123119" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/miaformer.png" alt="Visualization for MIA-Former">
            </div>
            <div class="card-body">
              <h3 class="paper-title">MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation</h3>
              <p class="authors">Zhongzhi Yu, Yonggan Fu, Sicheng Li, Mengquan Li, Chaojian Li, Yingyan (Celine) Lin</p>
              <p class="description">A multi-grained input-adaptive ViT that dynamically adjusts depth, heads, and tokens.</p>
              <a href="https://www.aaai.org/AAAI22Papers/AAAI-4994.YuZ.pdf" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
          <article class="publication-card">
            <div class="publication-media">
              <img src="images/LDP.png" alt="Visualization for LDP framework">
            </div>
            <div class="card-body">
              <h3 class="paper-title">LDP: Learnable Dynamic Precision for Efficient Deep Neural Network Training and Inference</h3>
              <p class="authors">Zhongzhi Yu, Yonggan Fu, Shang Wu, Mengquan Li, Haoran You, Yingyan Lin</p>
              <p class="description">Temporal and spatial precision scheduling for DNN training that optimizes accuracy-efficiency trade-offs.</p>
              <a href="https://arxiv.org/abs/2203.07713" target="_blank" rel="noopener" class="card-link">Paper</a>
            </div>
          </article>
        </div>
      </div>
    </section>

    <section class="section surface" id="service">
      <div class="container">
        <header class="section-header">
          <p class="section-kicker">Community</p>
          <h2>Academic Service</h2>
        </header>
        <div class="service-grid">
          <article class="service-card">
            <h3>Conference Reviewer</h3>
            <p>ICLR 2023, NeurIPS 2022–2023, ICML 2023, CVPR 2023, AAAI 2022, AICAS 2022.</p>
          </article>
          <article class="service-card">
            <h3>Artifact Evaluation</h3>
            <p>MICRO 2023 Artifact Evaluation Committee.</p>
          </article>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container footer-content">
      <div>
        <p class="footer-kicker">Last updated: Dec 2024</p>
      </div>
      <div class="footer-links">
        <a href="#top">Back to top</a>
        <a href="https://github.com/jonbarron/website" target="_blank" rel="noopener">Original Template</a>
      </div>
    </div>
  </footer>
</body>
</html>
